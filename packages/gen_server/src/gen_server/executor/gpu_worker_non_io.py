import asyncio
import multiprocessing
import queue
import traceback
from typing import (
    Any,
    AsyncGenerator,
    Type,
    TypeVar,
    Dict,
)

from multiprocessing import managers
from requests.packages.urllib3.util.retry import Retry
from PIL import PngImagePlugin
import torch
import struct

from ..base_types.pydantic_models import RunCommandConfig
from ..base_types import TorchDevice
from ..globals import (
    CustomNode,
    CheckpointMetadata,
    update_architectures,
    update_checkpoint_files,
    update_custom_nodes,
    set_available_torch_device,
)
from .workflows import generate_images_non_io
from ..config import set_config
from ..utils.file_handler import FileHandler, get_file_handler, FileURL
from ..utils.image import tensor_to_pil, tensor_to_bytes
from requests.adapters import HTTPAdapter

import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

T = TypeVar("T")

retry_strategy = Retry(
    total=3,
    backoff_factor=1,
    status_forcelist=[429, 500, 502, 503, 504],
)
adapter = HTTPAdapter(max_retries=retry_strategy)


async def upload_batch(
    file_handler: FileHandler, tensor_batch: torch.Tensor
) -> AsyncGenerator[FileURL, None]:
    metadata = PngImagePlugin.PngInfo()
    metadata.add_text("Description", "Generated by gen_server")
    metadata.add_text("Author", "gen_server")

    pil_images = tensor_to_pil(tensor_batch)

    logger.info("Starting to upload PNG files")
    async for file_url in file_handler.upload_png_files(pil_images, metadata):
        yield file_url


async def start_gpu_worker_non_io(
    task_queue: queue.Queue,
    cancel_registry: managers.DictProxy,
    cozy_config: RunCommandConfig,
    custom_nodes: Dict[str, Type[CustomNode]],
    checkpoint_files: Dict[str, CheckpointMetadata],
    architectures: Dict[str, Any],
    device: TorchDevice,
):
    manager = multiprocessing.Manager()

    set_config(cozy_config)
    update_custom_nodes(custom_nodes)
    update_architectures(architectures)
    update_checkpoint_files(checkpoint_files)
    set_available_torch_device(device)

    file_handler = get_file_handler(cozy_config)

    logger.info("GPU worker started")

    while True:
        try:
            data, response_conn, request_id = task_queue.get(timeout=1.0)
            if data is None:
                logger.info("Received stop signal. GPU-worker shutting down.")
                break

            if not isinstance(data, dict):
                logger.error(f"Invalid data received: {data}")
                if response_conn is not None:
                    response_conn.send(None)
                    response_conn.close()
                continue

            cancel_event = None
            if request_id is not None:
                cancel_event = manager.Event()
                cancel_registry[request_id] = cancel_event

                if cancel_event.is_set():
                    logger.info("Task was cancelled. Skipping task.")
                    continue

            try:
                for images in generate_images_non_io(data, cancel_event):
                    if cancel_event is not None and cancel_event.is_set():
                        raise asyncio.CancelledError("Operation was cancelled.")

                    image_bytes = tensor_to_bytes(images)
                    for image_bytes in image_bytes:
                        if response_conn is not None:
                            print("bytes sent", len(image_bytes))
                            header = struct.pack("!I", len(image_bytes))
                            response_conn.send(header + image_bytes)
            except asyncio.CancelledError:
                logger.info("Task was cancelled... Cancelling...")
                if response_conn is not None:
                    response_conn.send(None)
            except Exception as e:
                logger.error(f"Error in image generation: {str(e)}")
                if response_conn is not None:
                    response_conn.send(None)
            finally:
                if response_conn is not None:
                    response_conn.send(None)
                    response_conn.close()
        except queue.Empty:
            continue
        except Exception as e:
            traceback.print_exc()
            logger.error(f"Unexpected error in gpu-worker: {str(e)}")

    logger.info("GPU-worker shut down complete")


def start_gpu_worker(
    task_queue: queue.Queue,
    cancel_registry: managers.DictProxy,
    cozy_config: RunCommandConfig,
    custom_nodes: Dict[str, Type[CustomNode]],
    checkpoint_files: Dict[str, CheckpointMetadata],
    architectures: Dict[str, Any],
    device: TorchDevice,
):
    asyncio.run(
        start_gpu_worker_non_io(
            task_queue,
            cancel_registry,
            cozy_config,
            custom_nodes,
            checkpoint_files,
            architectures,
            device,
        )
    )


# async def run_worker(request_queue: multiprocessing.Queue):
#     with ThreadPoolExecutor() as executor:
#         while True:
#             if not request_queue.empty():
#                 (data, response_queue) = request_queue.get()
#                 if not isinstance(data, dict):
#                     print("Invalid data received")
#                     continue

#                 try:
#                     future = executor.submit(generate_images, **data)
#                     async for image_metadata in iterate_results(future):
#                         response_queue.put(image_metadata)
#                 except StopIteration:
#                     print("Images generated")

#             else:
#                 time.sleep(1)


# async def iterate_results(future: Future[AsyncGenerator[FileURL, None]]):
#     while not future.done():
#         await asyncio.sleep(0.1)

#     async for result in future.result():
#         yield result
